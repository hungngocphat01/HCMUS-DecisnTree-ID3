{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9ca58e",
   "metadata": {
    "id": "3b9ca58e"
   },
   "source": [
    "# Đồ án 03: Phân lớp và Gom nhóm\n",
    "\n",
    "- Họ tên: Hùng Ngọc Phát\n",
    "- MSSV: 19120615"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb7960",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9fcb7960",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82dc970622d3c0febfee4cee8df683fb",
     "grade": false,
     "grade_id": "project_content",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Nội dung đồ án\n",
    "\n",
    "Đồ án này hướng đến việc giúp các bạn thực hành và kiểm tra kiến thức về các thuật toán Phân lớp và Gom nhóm đã được học trong môn Khai thác Dữ liệu và Ứng dụng. \n",
    "\n",
    "Nội dung của đồ án này bao gồm:\n",
    "\n",
    "- Phân lớp dữ liệu nhị phân với Cây quyết định sử dụng thuật toán ID3\n",
    "- Gom nhóm dữ liệu với thuật toán k-means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XHGUv7o9ajw0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XHGUv7o9ajw0",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b785e40de1f2e077f34fc63b7945b7e",
     "grade": false,
     "grade_id": "project_instruction",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Hướng dẫn làm bài\n",
    "\n",
    "Các bạn cần hoàn thành đồ án bằng cách điền vào các ô code có ```YOUR CODE HERE``` hoặc các ô text có ```Your answer here``` \n",
    "\n",
    "<font color='red'>**LƯU Ý:**</font>\n",
    "\n",
    "- Bài làm của sinh viên phải do chính bản thân sinh viên tự làm, có thể trao đổi và tham khảo ý tưởng nhưng không được sao chép (một phần hoặc toàn bộ) code hoặc lời giải từ bất cứ người nào khác. Nếu vi phạm sẽ bị <font color='red'>0đ</font> bài tập này.\n",
    "- Các bạn có thể tạo thêm các cell trong quá trình code, tuy nhiên các bạn vui lòng <font color='red'>không xóa các cell code mặc định và các cell test case</font> (vì có thể ảnh hưởng đến kết quả khi chấm bài).\n",
    "- Các test case (nếu có) được đưa ra chỉ nhằm mục đích giúp các bạn test code của mình, việc pass các test case này không đồng nghĩa với việc lời giải của các bạn sẽ đạt điểm tối đa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36d4a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e0e36d4a",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6423679f1a3889c5b6bbe7cbb9316cc6",
     "grade": false,
     "grade_id": "project_submission",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Hướng dẫn nộp bài\n",
    "\n",
    "Khi chấm bài, đầu tiên mình sẽ chọn `Kernel` - `Restart Kernel & Run All Cells`, để restart và chạy tất cả các cell trong notebook của các bạn; do đó, trước khi nộp bài, các bạn nên chạy thử `Kernel` - `Restart Kernel & Run All Cells` để đảm bảo mọi chuyện diễn ra đúng như mong đợi.\n",
    "\n",
    "Sau đó, các bạn tạo thư mục nộp bài theo cấu trúc sau:\n",
    "\n",
    "- Thư mục `MSSV` (vd, nếu bạn có MSSV là 191235 thì bạn đặt tên thư mục là `191235`)\n",
    "    - ```data```: thư mục chứa các dữ liệu được cung cấp (và dữ liệu do bạn tìm thêm)\n",
    "    - ```[Lab 03 - Data Mining] Classification & Clustering.ipynb```: file bài làm của các bạn\n",
    "\n",
    "Cuối cùng, các bạn nén thư mục `MSSV` này lại và nộp ở link trên moodle **(Đuôi của file nén phải là .zip)**\n",
    "\n",
    "<font color=red>Các bạn lưu ý tuân thủ chính xác qui định nộp bài ở trên.</font>\n",
    "\n",
    "\n",
    "- Mọi thắc mắc trong quá trình làm bài, các bạn có thể tham khảo và điền vào [file Q&A](https://docs.google.com/spreadsheets/d/1BlUqNSl4Yn7LY1tNUM4gSRK3CbD8ZPhp37GVWXt-wmE/edit?usp=sharing), hoặc gửi mail đến  trợ giảng *Nguyễn Duy Khánh* (duykhanhnguyen360@gmail.com)\n",
    "\n",
    "\n",
    "<center><font color='green'>CHÚC CÁC BẠN HOÀN THÀNH THẬT TỐT ĐỒ ÁN!</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m2LENWZYHkli",
   "metadata": {
    "id": "m2LENWZYHkli"
   },
   "source": [
    "## Phần I: Phân lớp (7đ) + 1đ bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "An0DlQbBd1v_",
   "metadata": {
    "id": "An0DlQbBd1v_"
   },
   "source": [
    "### Yêu cầu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pgqzo20KivVh",
   "metadata": {
    "id": "Pgqzo20KivVh"
   },
   "source": [
    "Trong phần này, các bạn sẽ cần viết chương trình Python để cài đặt Cây quyết định sử dụng thuật toán ID3.\n",
    "\n",
    "Dữ liệu: \n",
    "\n",
    "- Với đồ án này, các bạn chỉ cần xây dựng mô hình cây quyết định với các thuộc tính phân loại (categorical), không cần xử lý thuộc tính số (numerical) (các bạn có thể chuyển các thuộc tính numerical trong input về dạng categorical trước khi đưa vào mô hình)\n",
    "- Mỗi điểm dữ liệu chỉ thuộc 1 trong 2 nhãn (label) cho trước (binary classification)\n",
    "- Tập dữ liệu sử dụng: [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing) và [titanic2.txt](https://drive.google.com/file/d/1IwOxcPz-Hq1_JOOfGylJxy1TYhDvfEsg/view?usp=sharing)\n",
    "\n",
    "\n",
    "Các bạn cần cài đặt hàm load dữ liệu và mô hình Cây quyết định với các phương thức sau:\n",
    "\n",
    "- ```fit```: xây dựng cây từ dữ liệu huấn luyện theo thuật toán ID3\n",
    "- ```predict```: dự đoán trên các điểm dữ liệu mới sử dụng cấu trúc cây đã xây dựng được từ dữ liệu huấn luyện\n",
    "- ```visualize```: vẽ cây quyết định đã xây dựng được từ tập dữ liệu huấn luyện. Các bạn có thể visualize cây quyết định đã xây dựng theo cách các bạn muốn, miễn sao có thể thấy được cấu trúc của cây (có thể tự implement hoặc sử dụng các package có sẵn như ```tree```, ```graphviz```, ...).  Ví dụ các bạn có thể visualize cây quyết định xây dựng được từ tập [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing) như sau:\n",
    "\n",
    "```\n",
    "outlook = sunny\n",
    "|  humidity = high: no\n",
    "|  humidity = normal: yes\n",
    "outlook = overcast: yes\n",
    "outlook = rainy\n",
    "|  windy = TRUE: no\n",
    "|  windy = FALSE: yes\n",
    "```\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- Với tập [tennis.txt](https://drive.google.com/file/d/1jv67IlMIxGxwGGaP47AZ_1t3mxIn82Y7/view?usp=sharing), sử dụng toàn bộ các điểm dữ liệu để làm tập train, không cần chia ra tập train và test. Với tập [titanic2.txt](https://drive.google.com/file/d/1IwOxcPz-Hq1_JOOfGylJxy1TYhDvfEsg/view?usp=sharing), các bạn sẽ cần chia ra tập train/test với tỉ lệ 80/20 (sử dụng ```train_test_split``` của ```sklearn``` với ```random_state=42```)\n",
    "- Hàm load dữ liệu và mô hình cây quyết định chỉ cài đặt 1 lần và phải sử dụng được cho các tập dữ liệu khác nhau\n",
    "- Với mỗi tập dữ liệu, các bạn cần ```load_data``` từ file input, ```fit``` mô hình với dữ liệu train, sử dụng mô hình huấn luyện để ```predict``` trên ```X_train```, ```X_test``` (nếu có) và tính độ chính xác (sử dụng hàm ```accuracy_score``` của ```sklearn```), cuối cùng là ```visualize``` mô hình đã huấn luyện\n",
    "- Sẽ có tối đa 1đ cộng nếu các bạn tự tìm và thực hiện tất cả các yêu cầu trên với tập dữ liệu mới"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ilCuHgbkd6w1",
   "metadata": {
    "id": "ilCuHgbkd6w1"
   },
   "source": [
    "### Cài đặt (6.5đ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "RD-kfiMzJA2g",
   "metadata": {
    "id": "RD-kfiMzJA2g"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "import graphviz as gv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RCUFoiMvAP3X",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1639881846113,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "RCUFoiMvAP3X",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcf3d778354bd11b9680231c3cfcf0de",
     "grade": false,
     "grade_id": "cell-f737deab76c4a037",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 1đ\n",
    "\n",
    "# Data loading and categorization (if necessary)\n",
    " \n",
    "def load_data(file_path, split=True):\n",
    "    \"\"\"\n",
    "    Load data from file path and return numpy data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path of input data file (tab separated).\n",
    "    split : bool\n",
    "        Whether or not to return test set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (X_train, y_train), (None, None): training numpy array if split = False, else\n",
    "    X_train, y_train, X_test, y_test: training and testing numpy array if split = True\n",
    "    \"\"\"    \n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    df = df.astype(str)\n",
    "    X = df.iloc[:, 0:-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33678d89-f080-46d4-88d5-fe4dade26bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A class representing a node in the decision tree.\n",
    "    \n",
    "    Supports more than 2 children.\n",
    "    Representing 2 children as an array also prettifies the code\n",
    "        without copy-pasting exactly same pieces of code for the left and right child.\n",
    "    \n",
    "    So consequentially, our tree can be n-ary (n can be > 2)!!! \n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        # Decision (class) of this node (if it is a leaf node)\n",
    "        self.decision = None \n",
    "        # List of rows which belong to this node (S set)\n",
    "        # Each element is a tuple (data, class), where `data` is `pd.Series` (one row in df)\n",
    "        self.items = list()\n",
    "        # List of links to its children\n",
    "        self.children = list()\n",
    "        # List of candidate attributes to take into account when splitting\n",
    "        self.candidate_attrs = list()\n",
    "        # Value of the split attribute of the parent node to jump into this node\n",
    "        self.jump_condition = None\n",
    "        # Node depth in tree (for prunning if needed)\n",
    "        self.depth = 0\n",
    "        # Split attribute of this node\n",
    "        self.split_attribute = None\n",
    "    \n",
    "    \"\"\"\n",
    "    @property decorator treat hàm này như một attribute và sẽ giúp ta \n",
    "    tiết kiệm được dấu ngoặc () khi gọi hàm vì hàm này cũng không có tham số.\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def is_leaf(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if this node is leaf node\n",
    "        \"\"\"\n",
    "        return (self.decision is not None)\n",
    "    \n",
    "    @property\n",
    "    def entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the entropy of this node\n",
    "        \"\"\"\n",
    "        # All classes of all items in this node\n",
    "        classes = np.array([c for (data, c) in self.items])\n",
    "        # Get counts\n",
    "        _, counts = np.unique(classes, return_counts=True)\n",
    "        # Calculate frequencies\n",
    "        frequencies = counts / len(self.items)\n",
    "        # Return the actual entropy\n",
    "        return -np.sum(frequencies * np.log2(frequencies))\n",
    "    \n",
    "    def generate_children_by_attr(self, attr: str):\n",
    "        \"\"\"\n",
    "        Set the split attribute of this node to `attr` and generate n children\n",
    "        where n is the number of values of attribute `attr` at this node.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "            - `attr`: column (attribute) name.\n",
    "        \"\"\"\n",
    "        # \"Clear\"\n",
    "        self.children = list()\n",
    "        \n",
    "        # All values of `attr` in the S set of this node\n",
    "        self.split_attribute = attr\n",
    "        values = np.unique([row[attr] for row, _ in self.items])\n",
    "        # Shrink candidate attribute list\n",
    "        subnode_attrs = [a for a in self.candidate_attrs if a != attr]\n",
    "        \n",
    "        for value in values:\n",
    "            child = Node()\n",
    "            child.jump_condition = value\n",
    "            child.items = [(row, c) for (row, c) in self.items if row[attr] == value]\n",
    "            child.depth = self.depth + 1\n",
    "            child.candidate_attrs = deepcopy(subnode_attrs)\n",
    "            \n",
    "            # If there is no item in this child (rare condition): treat this child as a leaf node\n",
    "            # the class will be the major class of this node \n",
    "            if len(child.items) == 0:\n",
    "                child.decision = self.major_class\n",
    "                \n",
    "            self.children.append(child)\n",
    "    \n",
    "    @property\n",
    "    def major_class(self) -> str:\n",
    "        \"\"\"\n",
    "        Return the major class of this node, which is the class that is owned by\n",
    "        the majority of items in this node.\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique([c for (row, c) in self.items], return_counts=True)\n",
    "        \n",
    "        mapping = {cl: count for (cl, count) in zip(classes, counts)}\n",
    "        return max(classes, key=lambda cl: mapping[cl])\n",
    "    \n",
    "    @property\n",
    "    def is_pure(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if current node is pure.\n",
    "        A node is pure when all of its items belong to only one class.\n",
    "        If this node is not pure but we have ran out of candidate attributes\n",
    "        to further split this node, we will \"pretend\" as if it is pure. \n",
    "\n",
    "        This function will also set `self.decision` to the appropiate value\n",
    "        in case this node is pure.\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique([c for (row, c) in self.items], return_counts=True)\n",
    "        \n",
    "        # If node is pure\n",
    "        if len(classes) == 1:\n",
    "            self.decision = classes[0]\n",
    "            return True\n",
    "        # If node is not pure but we had ran out of candidate columns\n",
    "        elif len(self.candidate_attrs) == 0:\n",
    "            self.decision = self.major_class\n",
    "            # Pretend as if this node is pure\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_info_gain(self, attr: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the information gain of this node against a specific attribute.\n",
    "        Gain(S, A) = Entropy(S) - sum{ |Sv|/|S| * Entropy(Sv) | v in A }\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "            - `attr`: attribute against which we calculate the info gain.\n",
    "        Return:\n",
    "        ------\n",
    "            - Info gain of this node respecting to `attr`.\n",
    "        \"\"\"\n",
    "        values = np.unique([row[attr] for row, _ in self.items])\n",
    "        \n",
    "        # Calculate the weighted entropy of this node (given that we are splitting using `attr`)\n",
    "        weighted_entropy = 0\n",
    "        for value in values:\n",
    "            temp_node = Node()\n",
    "            temp_node.items = [(row, c) for (row, c) in self.items if row[attr] == value]\n",
    "            weighted_entropy += (len(temp_node.items) / len(self.items)) * temp_node.entropy\n",
    "        return self.entropy - weighted_entropy\n",
    "    \n",
    "    def grow_subtree(self):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree using the ID3 algorithm\n",
    "        This is the \"real\" ID3 function.\n",
    "        \n",
    "        I had to split this into a separated function from DecisionTree.fit\n",
    "        because I implemented this in a recursive way.\n",
    "        \"\"\"\n",
    "        # Stop conditions\n",
    "        if self.is_leaf:\n",
    "            return \n",
    "        \n",
    "        # Calculate information gain of current node against all candidate attributes\n",
    "        gains = {attr: self.get_info_gain(attr) for attr in self.candidate_attrs}\n",
    "        \n",
    "        # Best attribute: attribute with highest gain\n",
    "        best_attr = max(gains, key=lambda attr: gains[attr])\n",
    "        \n",
    "        # Build child nodes for this node using the just found best attribute\n",
    "        self.generate_children_by_attr(best_attr)\n",
    "        \n",
    "        # Recursively build the subtree at each child of this node\n",
    "        for child in self.children:\n",
    "            if not child.is_pure:\n",
    "                child.grow_subtree()\n",
    "    \n",
    "    def predict(self, x_test: pd.Series):\n",
    "        \"\"\"\n",
    "        Function to predict the class of a new test example.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            - x_test: a `Series` (dataframe row) representing one test example.\n",
    "        Return:\n",
    "        -------\n",
    "            - The class of the given test example.\n",
    "        \"\"\"\n",
    "        if self.is_leaf:\n",
    "            return self.decision\n",
    "        \n",
    "        for child in self.children:\n",
    "            if x_test[self.split_attribute] == child.jump_condition:\n",
    "                return child.predict(x_test)\n",
    "        return random.choice(self.items)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a_uKLOc5jmjp",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1639881870624,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "a_uKLOc5jmjp",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a080f471787eae8322aa1a2cecca4536",
     "grade": false,
     "grade_id": "cell-b00859438bb62792",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 5đ\n",
    "\n",
    "# Decision Tree class\n",
    "# You should implement the ID3 algorithm here\n",
    "# You can add other utility methods to make your code easy to read :) \n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.root_node: Node\n",
    "\n",
    "    def _construct_root_node(self, X_train: pd.DataFrame, y_train):\n",
    "        \"\"\"\n",
    "        Internal function to load all examples into the root node of\n",
    "        this tree.\n",
    "        \"\"\"\n",
    "        self.root_node = Node()\n",
    "        self.root_node.items = [(row[1], y_train[row[0]]) for row in X_train.iterrows()]\n",
    "        self.root_node.candidate_attrs = [c for c in X_train.columns]\n",
    "        self.root_node.depth = 1                                 \n",
    "    \n",
    "    def fit(self, X_train: pd.DataFrame, y_train):\n",
    "        \"\"\"\n",
    "        Function to train a decision tree model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            - X_train: MUST BE A PANDAS DATAFRAME!!!\n",
    "            - y_train: any 1d iterable, but each element must be a `str`.\n",
    "        \"\"\"\n",
    "        self._construct_root_node(X_train, y_train)\n",
    "        self.root_node.grow_subtree()\n",
    "\n",
    "    def predict(self, X_test: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Proxy function to Node.predict, but is able to handle multiple rows.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            - X_test: test set (must be pd.DataFrame)\n",
    "        Return:\n",
    "        -------\n",
    "            - List of the predicted label of each row in X_test.\n",
    "        \"\"\"\n",
    "        return [self.root_node.predict(row[1]) for row in X_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "RKwazqgPAg2W",
   "metadata": {
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1639881962764,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "RKwazqgPAg2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "tree_tennis = DecisionTree()\n",
    "\n",
    "X, y = load_data(\"data/tennis.txt\", split=False)\n",
    "tree_tennis.fit(X, y)\n",
    "y_pred = tree_tennis.predict(X)\n",
    "print('Score:', accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073fa613-e0fa-406e-afad-72109966d999",
   "metadata": {
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1639881962764,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "RKwazqgPAg2W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7915151515151515\n",
      "Test score: 0.7876588021778584\n"
     ]
    }
   ],
   "source": [
    "tree_tinatic = DecisionTree()\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(\"data/titanic2.txt\")\n",
    "tree_tinatic.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = tree_tinatic.predict(X_train)\n",
    "print('Train score:', accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = tree_tinatic.predict(X_test)\n",
    "print('Test score:', accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7CXzAAx2Pt4N",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1639880484812,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "7CXzAAx2Pt4N",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88b654673e4ab9acaae48bc99ca22ec7",
     "grade": false,
     "grade_id": "cell-02d26fb701e008ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IHoz22FUVspR",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1639880500373,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "IHoz22FUVspR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94e9843cd7081e48d59f6ccb52e1dfcc",
     "grade": false,
     "grade_id": "cell-f6e050b570717c21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22t4pQ8wVtWH",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1639880509882,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "22t4pQ8wVtWH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "822db51073ce59cf1c3c1a01e487f90d",
     "grade": false,
     "grade_id": "cell-29a67030b178fde0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1seFAj_WVyRe",
   "metadata": {
    "deletable": false,
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1639880521270,
     "user": {
      "displayName": "Khánh Nguyễn",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW5QOJudmqsBqdJ5x2Hfp3zqPDQQRP2FGMhp4kpg=s64",
      "userId": "03950222018643951363"
     },
     "user_tz": -420
    },
    "id": "1seFAj_WVyRe",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81a03e536965535d3ee4d88b7c9c2f90",
     "grade": false,
     "grade_id": "cell-f5acd1c334d56c44",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NlmJn2FQPwSZ",
   "metadata": {
    "deletable": false,
    "id": "NlmJn2FQPwSZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e9c1924da30efe0c821b2f1a800834d",
     "grade": false,
     "grade_id": "cell-c210d26f0bf63b15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[Lab 03 - Data Mining] Classification & Clustering.ipynb",
   "provenance": [
    {
     "file_id": "1J1x4qE_dUH8LgPg7gLEh8H23DYDzNCki",
     "timestamp": 1639879447273
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
